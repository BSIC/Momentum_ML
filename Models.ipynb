{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RepeatedStratifiedKFold, cross_val_score, RepeatedKFold, RandomizedSearchCV, PredefinedSplit\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, confusion_matrix, classification_report, make_scorer, balanced_accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_signals = pd.read_csv('data/indicators.csv', parse_dates=True, index_col='Date')\n",
    "poly_features = pd.read_csv('data/indicators_w_polyterms.csv', parse_dates=True, index_col='Date')\n",
    "labels = pd.read_csv('data/Y_Matrix.csv', parse_dates=True, index_col='Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the period rolling window over which we train new models and how far back the training data goes\n",
    "\n",
    "window = 1\n",
    "lookback = 3\n",
    "\n",
    "# Creating a function to generate the training and testing data for the models, from 2000 onwards we create for every single year a train test split\n",
    "# by using last 3 years as training data and the next year as testing data\n",
    "# Eg. the model that will trade between 2000 and 2001 will be trained on data from 1997 to 1999 and tuned on 1999 to 2000 data (for the purposes of hyperparameter tuning)\n",
    "\n",
    "def generate_data(reg_signals, labels, window, lookback):\n",
    "    '''\n",
    "    Function to generate training and testing data for the models\n",
    "    \n",
    "    Parameters:\n",
    "    reg_signals: DataFrame containing the  signals\n",
    "    labels: DataFrame containing the labels\n",
    "    window: int, number of years to use for the testing data\n",
    "    lookback: int, number of years to use for the training data\n",
    "    \n",
    "    '''\n",
    "    X = reg_signals\n",
    "    y = labels\n",
    "    X_train = []\n",
    "    X_test = []\n",
    "    y_train = []\n",
    "    y_test = []\n",
    "    for i in range(2000, 2025):\n",
    "        date = datetime.strptime(str(i) + '-01-01', '%Y-%m-%d')\n",
    "        X_train.append(X.loc[str(date - pd.DateOffset(years=lookback)):str(date - pd.DateOffset(years=window))])\n",
    "        X_test.append(X.loc[str(date - pd.DateOffset(years=window)):str(date)])\n",
    "        y_train.append(y.loc[str(date - pd.DateOffset(years=lookback)):str(date - pd.DateOffset(years=window))])\n",
    "        y_test.append(y.loc[str(date - pd.DateOffset(years=window)):str(date)])\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = generate_data(reg_signals, labels, window, lookback)\n",
    "X_poly_train, X_poly_test, y_poly_train, y_poly_test = generate_data(poly_features, labels, window, lookback)\n",
    "\n",
    "# NB: y_train and y_poly_train are the same for all models, as the y_backtest and y_poly_backtest are the same for all models, so we just keep one set\n",
    "del y_poly_train, y_poly_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning function for one single pipeline for one single period. We use the same training and testing data for all models to ensure that the results are comparable.\n",
    "# We use the predefined X_train, X_test split instead of cross-validation to avoid look-ahead biases.\n",
    "\n",
    "def tune_model(X_train, X_test, y_train, y_test, params, pipeline, hyperparameter_tuner='grid', n_iter=None, verbose=1, sample_weights=None, probability=True):\n",
    "    '''\n",
    "    Function to tune a model using a predefined split\n",
    "    \n",
    "    Parameters:\n",
    "    X_train: DataFrame, training data\n",
    "    X_test: DataFrame, testing data over which we perform the hyperparameter tuning\n",
    "    y_train: DataFrame, training labels\n",
    "    y_test: DataFrame, testing labels\n",
    "    params: dict, hyperparameters to tune\n",
    "    pipeline: Pipeline, model to tune\n",
    "    hyperparameter_tuner: str, 'grid' or 'random', default='grid'\n",
    "    n_iter: int, number of iterations for random search, default=None\n",
    "    verbose: int, default=1\n",
    "    sample_weights: array-like, default=None\n",
    "    probability: bool, whether the model has a predict_proba method, default=True\n",
    "    \n",
    "    '''\n",
    "    # we first merge the training and testing data\n",
    "    X = pd.concat([X_train, X_test])\n",
    "    y = pd.concat([y_train, y_test])\n",
    "    # our crossvalidation cv uses predefined split to ensure that the training and testing data are actually maintained when tuning hyperparameters:\n",
    "    cv = PredefinedSplit(test_fold=[-1]*len(X_train) + [0]*len(X_test))\n",
    "     \n",
    "    if probability:\n",
    "        scoring = {'auc': make_scorer(roc_auc_score, needs_proba=True, multi_class=\"ovr\"), 'f1_macro': 'f1_macro', 'precision': 'precision_macro', 'recall': 'recall_macro', 'balanced_accuracy': 'balanced_accuracy'}\n",
    "    else: \n",
    "        scoring = {'f1_macro': 'f1_macro', 'precision': 'precision_macro', 'recall': 'recall_macro', 'balanced_accuracy': 'balanced_accuracy'}\n",
    "    principal_metric = 'balanced_accuracy'\n",
    "\n",
    "    if hyperparameter_tuner == 'grid':\n",
    "        search = GridSearchCV(pipeline, params, cv=cv, scoring=scoring, refit=principal_metric, n_jobs=-1, verbose=verbose, error_score='raise')\n",
    "    elif hyperparameter_tuner == 'random':\n",
    "        search = RandomizedSearchCV(pipeline, params, cv=cv, scoring=scoring, refit=principal_metric, n_iter=n_iter, n_jobs=-1, verbose=verbose)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown hyperparameter tuner: {hyperparameter_tuner}\\n Choose from 'grid', 'random'\")\n",
    "    if sample_weights is not None:\n",
    "        kwargs = {pipeline.steps[-1][0] + '__sample_weight': sample_weights}\n",
    "        result = search.fit(X, y, **kwargs)\n",
    "    else:\n",
    "        result = search.fit(X, y)\n",
    "    best_model = result.best_estimator_\n",
    "\n",
    "    return result, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_validation_performance(result, probability=True):\n",
    "    print('')\n",
    "    print(f\"Cross-validation balanced accuracy scores: {result.best_score_}\\n\")\n",
    "    print(f\"Cross-validation F1 Macro scores: {result.cv_results_['mean_test_f1_macro'][result.best_index_]}\\n\")\n",
    "    if probability:\n",
    "        print(f\"Cross-validation ROC AUC scores: {result.cv_results_['mean_test_auc'][result.best_index_]}\\n\")\n",
    "    print(f\"Cross-validation precision scores: {result.cv_results_['mean_test_precision'][result.best_index_]}\\n\")\n",
    "    print(f\"Cross-validation recall scores: {result.cv_results_['mean_test_recall'][result.best_index_]}\\n\")\n",
    "    print(f\"Parameters for the best model: \\n{result.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_model_tuning(X_train, X_test, y_train, y_test, params, pipeline, hyperparameter_tuner='grid', n_iter=None, verbose=1, sample_weights=None, probability=True):\n",
    "    '''\n",
    "    Function to tune a model using a rolling window\n",
    "    \n",
    "    Parameters:\n",
    "    X_train: list, training data\n",
    "    X_test: list, testing data over which we perform the hyperparameter tuning\n",
    "    y_train: list, training labels\n",
    "    y_test: list, testing labels\n",
    "    params: dict, hyperparameters to tune\n",
    "    pipeline: Pipeline, model to tune\n",
    "    hyperparameter_tuner: str, 'grid' or 'random', default='grid'\n",
    "    n_iter: int, number of iterations for random search, default=None\n",
    "    verbose: int, default=1\n",
    "    sample_weights: array-like, default=None\n",
    "    probability: bool, whether the model has a predict_proba method, default=True\n",
    "    \n",
    "    '''\n",
    "    results = dict()\n",
    "    best_models = dict()\n",
    "    for target in y_train[0].columns:\n",
    "        print(f\"Training model for target {target}\\n\")\n",
    "        for i in range(len(X_train)):\n",
    "            result, best_model = tune_model(X_train[i], X_test[i], y_train[i][target], y_test[i][target], params, pipeline, hyperparameter_tuner=hyperparameter_tuner, n_iter=n_iter, verbose=verbose, sample_weights=sample_weights, probability=probability)\n",
    "            results[target] = result\n",
    "            best_models[target] = best_model\n",
    "            # we print the results for each model\n",
    "            print(f\"Results for model {i+2000}:\")\n",
    "            if verbose:\n",
    "                model_validation_performance(result, probability=probability)\n",
    "            else:\n",
    "                print(f\"Best score: {result.best_score_}\")\n",
    "        print('\\n\\n\\n')\n",
    "    return results, best_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "log_reg_pipeline = Pipeline(\n",
    "    [('log_reg', LogisticRegression(max_iter=1000))]\n",
    ")\n",
    "\n",
    "# Define the parameters to tune\n",
    "log_reg_params = [{\n",
    "    'log_reg__C': [0.01, 0.1, 1, 10, 100],\n",
    "    'log_reg__solver': ['liblinear'],\n",
    "    'log_reg__penalty': ['l1', 'l2'],\n",
    "    'log_reg__class_weight': ['balanced', None]\n",
    "},\n",
    "                  {\n",
    "    'log_reg__C': [0.01, 0.1, 1, 10, 100],\n",
    "    'log_reg__solver': ['saga'],\n",
    "    'log_reg__penalty': ['elasticnet'],\n",
    "    'log_reg__l1_ratio': [0.1, 0.5, 0.9],\n",
    "    'log_reg__class_weight': ['balanced', None]\n",
    "                  }]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for target long 3 days\n",
      "\n",
      "Results for model 2000:\n",
      "Best score: 0.6472222222222221\n",
      "Results for model 2001:\n",
      "Best score: 0.6481481481481481\n",
      "Results for model 2002:\n",
      "Best score: 0.5743194192377495\n",
      "Results for model 2003:\n",
      "Best score: 0.6077712609970675\n",
      "Results for model 2004:\n",
      "Best score: 0.5634056299030918\n",
      "Results for model 2005:\n",
      "Best score: 0.5788785934901401\n",
      "Results for model 2006:\n",
      "Best score: 0.6085714285714285\n",
      "Results for model 2007:\n",
      "Best score: 0.5418939393939394\n",
      "Results for model 2008:\n",
      "Best score: 0.5944720056697378\n",
      "Results for model 2009:\n",
      "Best score: 0.5685393258426966\n",
      "Results for model 2010:\n",
      "Best score: 0.6228146853146853\n",
      "Results for model 2011:\n",
      "Best score: 0.6553813257305774\n",
      "Results for model 2012:\n",
      "Best score: 0.6174418604651163\n",
      "Results for model 2013:\n",
      "Best score: 0.6175\n",
      "Results for model 2014:\n",
      "Best score: 0.5718077474892396\n",
      "Results for model 2015:\n",
      "Best score: 0.6296919587327139\n",
      "Results for model 2016:\n",
      "Best score: 0.6851983844143502\n",
      "Results for model 2017:\n",
      "Best score: 0.6178451178451179\n",
      "Results for model 2018:\n",
      "Best score: 0.5871212121212122\n",
      "Results for model 2019:\n",
      "Best score: 0.5804589406840814\n",
      "Results for model 2020:\n",
      "Best score: 0.678979843685726\n",
      "Results for model 2021:\n",
      "Best score: 0.7078549559892844\n",
      "Results for model 2022:\n",
      "Best score: 0.715113726178976\n",
      "Results for model 2023:\n",
      "Best score: 0.5770344287949922\n",
      "Results for model 2024:\n",
      "Best score: 0.5495024021962938\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training model for target short 3 days\n",
      "\n",
      "Results for model 2000:\n",
      "Best score: 0.5610356536502547\n",
      "Results for model 2001:\n",
      "Best score: 0.5648610121168924\n",
      "Results for model 2002:\n",
      "Best score: 0.5058134740661334\n",
      "Results for model 2003:\n",
      "Best score: 0.5286062081558125\n",
      "Results for model 2004:\n",
      "Best score: 0.5\n",
      "Results for model 2005:\n",
      "Best score: 0.5318877551020409\n",
      "Results for model 2006:\n",
      "Best score: 0.5076530612244898\n",
      "Results for model 2007:\n",
      "Best score: 0.5\n",
      "Results for model 2008:\n",
      "Best score: 0.5179186228482003\n",
      "Results for model 2009:\n",
      "Best score: 0.5487674883411059\n",
      "Results for model 2010:\n",
      "Best score: 0.6949308755760368\n",
      "Results for model 2011:\n",
      "Best score: 0.5896153846153847\n",
      "Results for model 2012:\n",
      "Best score: 0.5546675191815856\n",
      "Results for model 2013:\n",
      "Best score: 0.5134598411297441\n",
      "Results for model 2014:\n",
      "Best score: 0.5190476190476191\n",
      "Results for model 2015:\n",
      "Best score: 0.6078431372549019\n",
      "Results for model 2016:\n",
      "Best score: 0.5431218816821097\n",
      "Results for model 2017:\n",
      "Best score: 0.6134259259259259\n",
      "Results for model 2018:\n",
      "Best score: 0.5580947136563876\n",
      "Results for model 2019:\n",
      "Best score: 0.5757367537028555\n",
      "Results for model 2020:\n",
      "Best score: 0.6393261190359075\n",
      "Results for model 2021:\n",
      "Best score: 0.6985645933014355\n",
      "Results for model 2022:\n",
      "Best score: 0.603050649540213\n",
      "Results for model 2023:\n",
      "Best score: 0.5465545529122231\n",
      "Results for model 2024:\n",
      "Best score: 0.5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training model for target long 5 days\n",
      "\n",
      "Results for model 2000:\n",
      "Best score: 0.6147291539866098\n",
      "Results for model 2001:\n",
      "Best score: 0.6655124114854876\n",
      "Results for model 2002:\n",
      "Best score: 0.5881272949816401\n",
      "Results for model 2003:\n",
      "Best score: 0.5817359111476759\n",
      "Results for model 2004:\n",
      "Best score: 0.5015146178231772\n",
      "Results for model 2005:\n",
      "Best score: 0.5829268292682926\n",
      "Results for model 2006:\n",
      "Best score: 0.5738264286206659\n",
      "Results for model 2007:\n",
      "Best score: 0.5646864686468647\n",
      "Results for model 2008:\n",
      "Best score: 0.6332013201320132\n",
      "Results for model 2009:\n",
      "Best score: 0.5760389036251106\n",
      "Results for model 2010:\n",
      "Best score: 0.6020084112951678\n",
      "Results for model 2011:\n",
      "Best score: 0.574320652173913\n",
      "Results for model 2012:\n",
      "Best score: 0.6378769515933557\n",
      "Results for model 2013:\n",
      "Best score: 0.6069563882063882\n",
      "Results for model 2014:\n",
      "Best score: 0.5923526015269135\n",
      "Results for model 2015:\n",
      "Best score: 0.6428999002327902\n",
      "Results for model 2016:\n",
      "Best score: 0.6748923959827833\n",
      "Results for model 2017:\n",
      "Best score: 0.6361626878868258\n",
      "Results for model 2018:\n",
      "Best score: 0.5841972355130249\n",
      "Results for model 2019:\n",
      "Best score: 0.5747757397241933\n",
      "Results for model 2020:\n",
      "Best score: 0.6264473684210526\n",
      "Results for model 2021:\n",
      "Best score: 0.6171794871794871\n",
      "Results for model 2022:\n",
      "Best score: 0.6668897499119408\n",
      "Results for model 2023:\n",
      "Best score: 0.549273783587509\n",
      "Results for model 2024:\n",
      "Best score: 0.5731617647058823\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training model for target short 5 days\n",
      "\n",
      "Results for model 2000:\n",
      "Best score: 0.5943304007820137\n",
      "Results for model 2001:\n",
      "Best score: 0.5764945652173913\n",
      "Results for model 2002:\n",
      "Best score: 0.5168526702119691\n",
      "Results for model 2003:\n",
      "Best score: 0.5204081632653061\n",
      "Results for model 2004:\n",
      "Best score: 0.5\n",
      "Results for model 2005:\n",
      "Best score: 0.516248457424928\n",
      "Results for model 2006:\n",
      "Best score: 0.6065934065934065\n",
      "Results for model 2007:\n",
      "Best score: 0.5\n",
      "Results for model 2008:\n",
      "Best score: 0.5100778744846541\n",
      "Results for model 2009:\n",
      "Best score: 0.5328181453231633\n",
      "Results for model 2010:\n",
      "Best score: 0.7003236620043926\n",
      "Results for model 2011:\n",
      "Best score: 0.5703874866690366\n",
      "Results for model 2012:\n",
      "Best score: 0.566728452270621\n",
      "Results for model 2013:\n",
      "Best score: 0.53\n",
      "Results for model 2014:\n",
      "Best score: 0.5\n",
      "Results for model 2015:\n",
      "Best score: 0.5828125\n",
      "Results for model 2016:\n",
      "Best score: 0.5302156088672942\n",
      "Results for model 2017:\n",
      "Best score: 0.6188725490196079\n",
      "Results for model 2018:\n",
      "Best score: 0.5200884955752213\n",
      "Results for model 2019:\n",
      "Best score: 0.5650526771539905\n",
      "Results for model 2020:\n",
      "Best score: 0.6231649520286672\n",
      "Results for model 2021:\n",
      "Best score: 0.6951231527093595\n",
      "Results for model 2022:\n",
      "Best score: 0.5069444444444444\n",
      "Results for model 2023:\n",
      "Best score: 0.5050505050505051\n",
      "Results for model 2024:\n",
      "Best score: 0.5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training model for target long 8 days\n",
      "\n",
      "Results for model 2000:\n",
      "Best score: 0.5594117647058824\n",
      "Results for model 2001:\n",
      "Best score: 0.6180936764810723\n",
      "Results for model 2002:\n",
      "Best score: 0.5403715077522283\n",
      "Results for model 2003:\n",
      "Best score: 0.5826967642262466\n",
      "Results for model 2004:\n",
      "Best score: 0.5\n",
      "Results for model 2005:\n",
      "Best score: 0.5510458330601272\n",
      "Results for model 2006:\n",
      "Best score: 0.5594117647058824\n",
      "Results for model 2007:\n",
      "Best score: 0.5835254193878858\n",
      "Results for model 2008:\n",
      "Best score: 0.5061343782117163\n",
      "Results for model 2009:\n",
      "Best score: 0.5768156424581006\n",
      "Results for model 2010:\n",
      "Best score: 0.5007173601147776\n",
      "Results for model 2011:\n",
      "Best score: 0.5357664233576642\n",
      "Results for model 2012:\n",
      "Best score: 0.6410018552875696\n",
      "Results for model 2013:\n",
      "Best score: 0.5336805555555555\n",
      "Results for model 2014:\n",
      "Best score: 0.5813799203892083\n",
      "Results for model 2015:\n",
      "Best score: 0.635990322786019\n",
      "Results for model 2016:\n",
      "Best score: 0.678125\n",
      "Results for model 2017:\n",
      "Best score: 0.6098901098901098\n",
      "Results for model 2018:\n",
      "Best score: 0.5937857415173465\n",
      "Results for model 2019:\n",
      "Best score: 0.5354526425954997\n",
      "Results for model 2020:\n",
      "Best score: 0.6041889763779528\n",
      "Results for model 2021:\n",
      "Best score: 0.596042471042471\n",
      "Results for model 2022:\n",
      "Best score: 0.6255263157894737\n",
      "Results for model 2023:\n",
      "Best score: 0.572692036645525\n",
      "Results for model 2024:\n",
      "Best score: 0.5311206004140787\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training model for target short 8 days\n",
      "\n",
      "Results for model 2000:\n",
      "Best score: 0.5692307692307692\n",
      "Results for model 2001:\n",
      "Best score: 0.5851434096036094\n",
      "Results for model 2002:\n",
      "Best score: 0.5\n",
      "Results for model 2003:\n",
      "Best score: 0.5173789173789174\n",
      "Results for model 2004:\n",
      "Best score: 0.5\n",
      "Results for model 2005:\n",
      "Best score: 0.5398134863701578\n",
      "Results for model 2006:\n",
      "Best score: 0.580699471952246\n",
      "Results for model 2007:\n",
      "Best score: 0.5\n",
      "Results for model 2008:\n",
      "Best score: 0.5224569640062597\n",
      "Results for model 2009:\n",
      "Best score: 0.5155228758169934\n",
      "Results for model 2010:\n",
      "Best score: 0.7\n",
      "Results for model 2011:\n",
      "Best score: 0.5467741935483871\n",
      "Results for model 2012:\n",
      "Best score: 0.5864760818421615\n",
      "Results for model 2013:\n",
      "Best score: 0.5578589108910892\n",
      "Results for model 2014:\n",
      "Best score: 0.5\n",
      "Results for model 2015:\n",
      "Best score: 0.5695246179966045\n",
      "Results for model 2016:\n",
      "Best score: 0.5812215724496426\n",
      "Results for model 2017:\n",
      "Best score: 0.5600490196078431\n",
      "Results for model 2018:\n",
      "Best score: 0.5302052785923753\n",
      "Results for model 2019:\n",
      "Best score: 0.5636528685548294\n",
      "Results for model 2020:\n",
      "Best score: 0.6487115021998743\n",
      "Results for model 2021:\n",
      "Best score: 0.7350267891312667\n",
      "Results for model 2022:\n",
      "Best score: 0.5441749193279182\n",
      "Results for model 2023:\n",
      "Best score: 0.5971898471898471\n",
      "Results for model 2024:\n",
      "Best score: 0.5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training model for target long 10 days\n",
      "\n",
      "Results for model 2000:\n",
      "Best score: 0.5212765957446808\n",
      "Results for model 2001:\n",
      "Best score: 0.5834900731452456\n",
      "Results for model 2002:\n",
      "Best score: 0.5720238095238095\n",
      "Results for model 2003:\n",
      "Best score: 0.5671355498721227\n",
      "Results for model 2004:\n",
      "Best score: 0.5103950103950103\n",
      "Results for model 2005:\n",
      "Best score: 0.5961842105263158\n",
      "Results for model 2006:\n",
      "Best score: 0.6078184110970997\n",
      "Results for model 2007:\n",
      "Best score: 0.5425079365079365\n",
      "Results for model 2008:\n",
      "Best score: 0.5\n",
      "Results for model 2009:\n",
      "Best score: 0.5084670231729055\n",
      "Results for model 2010:\n",
      "Best score: 0.5\n",
      "Results for model 2011:\n",
      "Best score: 0.5144927536231884\n",
      "Results for model 2012:\n",
      "Best score: 0.6948684210526316\n",
      "Results for model 2013:\n",
      "Best score: 0.5531972239067449\n",
      "Results for model 2014:\n",
      "Best score: 0.5459212598425196\n",
      "Results for model 2015:\n",
      "Best score: 0.6273941532258065\n",
      "Results for model 2016:\n",
      "Best score: 0.6759259259259259\n",
      "Results for model 2017:\n",
      "Best score: 0.62708719851577\n",
      "Results for model 2018:\n",
      "Best score: 0.5777222008464794\n",
      "Results for model 2019:\n",
      "Best score: 0.5718199608610568\n",
      "Results for model 2020:\n",
      "Best score: 0.6857142857142857\n",
      "Results for model 2021:\n",
      "Best score: 0.5993541033434651\n",
      "Results for model 2022:\n",
      "Best score: 0.5741071428571428\n",
      "Results for model 2023:\n",
      "Best score: 0.5702420016406891\n",
      "Results for model 2024:\n",
      "Best score: 0.5191070509521358\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training model for target short 10 days\n",
      "\n",
      "Results for model 2000:\n",
      "Best score: 0.5475172249940603\n",
      "Results for model 2001:\n",
      "Best score: 0.5456363517146416\n",
      "Results for model 2002:\n",
      "Best score: 0.5260431058303399\n",
      "Results for model 2003:\n",
      "Best score: 0.5385826771653544\n",
      "Results for model 2004:\n",
      "Best score: 0.5\n",
      "Results for model 2005:\n",
      "Best score: 0.5094817138375989\n",
      "Results for model 2006:\n",
      "Best score: 0.5939389301293334\n",
      "Results for model 2007:\n",
      "Best score: 0.5447549019607842\n",
      "Results for model 2008:\n",
      "Best score: 0.5177389564414345\n",
      "Results for model 2009:\n",
      "Best score: 0.5619494085074251\n",
      "Results for model 2010:\n",
      "Best score: 0.6595238095238095\n",
      "Results for model 2011:\n",
      "Best score: 0.5870027330701488\n",
      "Results for model 2012:\n",
      "Best score: 0.5550215208034434\n",
      "Results for model 2013:\n",
      "Best score: 0.5500882612533098\n",
      "Results for model 2014:\n",
      "Best score: 0.5\n",
      "Results for model 2015:\n",
      "Best score: 0.5342420212765957\n",
      "Results for model 2016:\n",
      "Best score: 0.5523510971786834\n",
      "Results for model 2017:\n",
      "Best score: 0.5\n",
      "Results for model 2018:\n",
      "Best score: 0.5281084656084656\n",
      "Results for model 2019:\n",
      "Best score: 0.5869908204245553\n",
      "Results for model 2020:\n",
      "Best score: 0.6834872593746523\n",
      "Results for model 2021:\n",
      "Best score: 0.7793103448275862\n",
      "Results for model 2022:\n",
      "Best score: 0.532683846637335\n",
      "Results for model 2023:\n",
      "Best score: 0.542160989922184\n",
      "Results for model 2024:\n",
      "Best score: 0.5034087810199073\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_reg_results, log_reg_best_models = rolling_model_tuning(X_train, X_test, y_train, y_test, log_reg_params, log_reg_pipeline, hyperparameter_tuner='grid', verbose=0, probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as a pickle file in the models folder\n",
    "\n",
    "with open('models/log_reg_results.pkl', 'wb') as f:\n",
    "    pickle.dump(log_reg_results, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "rf_pipeline = Pipeline(\n",
    "    [('rf', RandomForestClassifier())]\n",
    ")\n",
    "\n",
    "# Define the parameters to tune\n",
    "rf_params = {\n",
    "    'rf__n_estimators': [100, 200, 300, 400, 500],\n",
    "    'rf__max_depth': [10, 20, 30, 40, 50],\n",
    "    'rf__min_samples_split': [2, 5, 10],\n",
    "    'rf__min_samples_leaf': [1, 2, 4],\n",
    "    'rf__bootstrap': [True, False],\n",
    "    'rf__class_weight': ['balanced', None]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for target long 3 days\n",
      "\n",
      "Results for model 2000:\n",
      "Best score: 0.6194444444444445\n",
      "Results for model 2001:\n",
      "Best score: 0.5793650793650793\n",
      "Results for model 2002:\n",
      "Best score: 0.619600725952813\n",
      "Results for model 2003:\n",
      "Best score: 0.5720918866080157\n",
      "Results for model 2004:\n",
      "Best score: 0.5335948315643747\n",
      "Results for model 2005:\n",
      "Best score: 0.5837491090520314\n",
      "Results for model 2006:\n",
      "Best score: 0.5784415584415584\n",
      "Results for model 2007:\n",
      "Best score: 0.5663636363636364\n",
      "Results for model 2008:\n",
      "Best score: 0.609355067328136\n",
      "Results for model 2009:\n",
      "Best score: 0.6107865168539326\n",
      "Results for model 2010:\n",
      "Best score: 0.5321241258741258\n",
      "Results for model 2011:\n",
      "Best score: 0.5838679021145166\n",
      "Results for model 2012:\n",
      "Best score: 0.6072674418604651\n",
      "Results for model 2013:\n",
      "Best score: 0.565\n",
      "Results for model 2014:\n",
      "Best score: 0.5586800573888092\n",
      "Results for model 2015:\n",
      "Best score: 0.6371551913367967\n",
      "Results for model 2016:\n",
      "Best score: 0.675694939415538\n",
      "Results for model 2017:\n",
      "Best score: 0.5951178451178452\n",
      "Results for model 2018:\n",
      "Best score: 0.5524621212121212\n",
      "Results for model 2019:\n",
      "Best score: 0.5406624332515515\n",
      "Results for model 2020:\n",
      "Best score: 0.6244343891402715\n",
      "Results for model 2021:\n",
      "Best score: 0.5997895139686185\n",
      "Results for model 2022:\n",
      "Best score: 0.6301484148590498\n",
      "Results for model 2023:\n",
      "Best score: 0.5568857589984351\n",
      "Results for model 2024:\n",
      "Best score: 0.5514756348661634\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training model for target short 3 days\n",
      "\n",
      "Results for model 2000:\n",
      "Best score: 0.5611205432937182\n",
      "Results for model 2001:\n",
      "Best score: 0.5342124019957235\n",
      "Results for model 2002:\n",
      "Best score: 0.5282015337676259\n",
      "Results for model 2003:\n",
      "Best score: 0.5401704199634815\n",
      "Results for model 2004:\n",
      "Best score: 0.4932432432432432\n",
      "Results for model 2005:\n",
      "Best score: 0.5369897959183674\n",
      "Results for model 2006:\n",
      "Best score: 0.5063775510204082\n",
      "Results for model 2007:\n",
      "Best score: 0.4978278440088907\n",
      "Results for model 2008:\n",
      "Best score: 0.520226917057903\n",
      "Results for model 2009:\n",
      "Best score: 0.5925383077948034\n",
      "Results for model 2010:\n",
      "Best score: 0.5262672811059907\n",
      "Results for model 2011:\n",
      "Best score: 0.5\n",
      "Results for model 2012:\n",
      "Best score: 0.5425191815856777\n",
      "Results for model 2013:\n",
      "Best score: 0.5885922330097088\n",
      "Results for model 2014:\n",
      "Best score: 0.5428571428571429\n",
      "Results for model 2015:\n",
      "Best score: 0.5667892156862745\n",
      "Results for model 2016:\n",
      "Best score: 0.5\n",
      "Results for model 2017:\n",
      "Best score: 0.5625\n",
      "Results for model 2018:\n",
      "Best score: 0.6007709251101321\n",
      "Results for model 2019:\n",
      "Best score: 0.5493968544816002\n",
      "Results for model 2020:\n",
      "Best score: 0.5541072306935564\n",
      "Results for model 2021:\n",
      "Best score: 0.6070574162679426\n",
      "Results for model 2022:\n",
      "Best score: 0.5314552620055466\n",
      "Results for model 2023:\n",
      "Best score: 0.5397867104183758\n",
      "Results for model 2024:\n",
      "Best score: 0.5142857142857142\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training model for target long 5 days\n",
      "\n",
      "Results for model 2000:\n",
      "Best score: 0.7021708257252992\n",
      "Results for model 2001:\n",
      "Best score: 0.5508909812465956\n",
      "Results for model 2002:\n",
      "Best score: 0.6178090575275398\n",
      "Results for model 2003:\n",
      "Best score: 0.581324557795146\n",
      "Results for model 2004:\n",
      "Best score: 0.5264177527298344\n",
      "Results for model 2005:\n",
      "Best score: 0.5875896700143473\n",
      "Results for model 2006:\n",
      "Best score: 0.5642448473150893\n",
      "Results for model 2007:\n",
      "Best score: 0.5434653465346535\n",
      "Results for model 2008:\n",
      "Best score: 0.5610561056105611\n",
      "Results for model 2009:\n",
      "Best score: 0.657603890362511\n",
      "Results for model 2010:\n",
      "Best score: 0.5606814865676766\n",
      "Results for model 2011:\n",
      "Best score: 0.5398097826086956\n",
      "Results for model 2012:\n",
      "Best score: 0.6051899907321594\n",
      "Results for model 2013:\n",
      "Best score: 0.5548986486486487\n",
      "Results for model 2014:\n",
      "Best score: 0.5847501122730481\n",
      "Results for model 2015:\n",
      "Best score: 0.6925174592617227\n",
      "Results for model 2016:\n",
      "Best score: 0.6619081779053084\n",
      "Results for model 2017:\n",
      "Best score: 0.6518567639257294\n",
      "Results for model 2018:\n",
      "Best score: 0.6059276980329612\n",
      "Results for model 2019:\n",
      "Best score: 0.5451533003079394\n",
      "Results for model 2020:\n",
      "Best score: 0.6223684210526316\n",
      "Results for model 2021:\n",
      "Best score: 0.5445054945054946\n",
      "Results for model 2022:\n",
      "Best score: 0.6828108488904544\n",
      "Results for model 2023:\n",
      "Best score: 0.5033405954974582\n",
      "Results for model 2024:\n",
      "Best score: 0.5702205882352941\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training model for target short 5 days\n",
      "\n",
      "Results for model 2000:\n",
      "Best score: 0.5510752688172043\n",
      "Results for model 2001:\n",
      "Best score: 0.5402173913043478\n",
      "Results for model 2002:\n",
      "Best score: 0.5378248820313085\n",
      "Results for model 2003:\n",
      "Best score: 0.5319727891156463\n",
      "Results for model 2004:\n",
      "Best score: 0.5484068627450981\n",
      "Results for model 2005:\n",
      "Best score: 0.5716166186754422\n",
      "Results for model 2006:\n",
      "Best score: 0.5104395604395604\n",
      "Results for model 2007:\n",
      "Best score: 0.5053633552377271\n",
      "Results for model 2008:\n",
      "Best score: 0.5137807298824247\n",
      "Results for model 2009:\n",
      "Best score: 0.5502810116419109\n",
      "Results for model 2010:\n",
      "Best score: 0.5942665587793319\n",
      "Results for model 2011:\n",
      "Best score: 0.5\n",
      "Results for model 2012:\n",
      "Best score: 0.5218863620161118\n",
      "Results for model 2013:\n",
      "Best score: 0.585\n",
      "Results for model 2014:\n",
      "Best score: 0.5708801602314454\n",
      "Results for model 2015:\n",
      "Best score: 0.5651041666666666\n",
      "Results for model 2016:\n",
      "Best score: 0.5360613422411175\n",
      "Results for model 2017:\n",
      "Best score: 0.5520833333333334\n",
      "Results for model 2018:\n",
      "Best score: 0.5203539823008849\n",
      "Results for model 2019:\n",
      "Best score: 0.5277096262086881\n",
      "Results for model 2020:\n",
      "Best score: 0.5854236504450353\n",
      "Results for model 2021:\n",
      "Best score: 0.5639408866995074\n",
      "Results for model 2022:\n",
      "Best score: 0.49074074074074076\n",
      "Results for model 2023:\n",
      "Best score: 0.5057482721956406\n",
      "Results for model 2024:\n",
      "Best score: 0.511615515771526\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training model for target long 8 days\n",
      "\n",
      "Results for model 2000:\n",
      "Best score: 0.5527450980392157\n",
      "Results for model 2001:\n",
      "Best score: 0.5799529478862194\n",
      "Results for model 2002:\n",
      "Best score: 0.5153172047037675\n",
      "Results for model 2003:\n",
      "Best score: 0.6112350871169856\n",
      "Results for model 2004:\n",
      "Best score: 0.5333333333333333\n",
      "Results for model 2005:\n",
      "Best score: 0.6597272310012459\n",
      "Results for model 2006:\n",
      "Best score: 0.5566666666666666\n",
      "Results for model 2007:\n",
      "Best score: 0.6039505698552952\n",
      "Results for model 2008:\n",
      "Best score: 0.4608812949640288\n",
      "Results for model 2009:\n",
      "Best score: 0.5780613015249887\n",
      "Results for model 2010:\n",
      "Best score: 0.5584648493543759\n",
      "Results for model 2011:\n",
      "Best score: 0.4998095842589654\n",
      "Results for model 2012:\n",
      "Best score: 0.5797773654916512\n",
      "Results for model 2013:\n",
      "Best score: 0.4934027777777778\n",
      "Results for model 2014:\n",
      "Best score: 0.5776205218929678\n",
      "Results for model 2015:\n",
      "Best score: 0.7009931877506843\n",
      "Results for model 2016:\n",
      "Best score: 0.6831521739130435\n",
      "Results for model 2017:\n",
      "Best score: 0.6249402771141902\n",
      "Results for model 2018:\n",
      "Best score: 0.56941796924641\n",
      "Results for model 2019:\n",
      "Best score: 0.5550431711145997\n",
      "Results for model 2020:\n",
      "Best score: 0.5928188976377953\n",
      "Results for model 2021:\n",
      "Best score: 0.5968146718146718\n",
      "Results for model 2022:\n",
      "Best score: 0.6339473684210526\n",
      "Results for model 2023:\n",
      "Best score: 0.49429175475687104\n",
      "Results for model 2024:\n",
      "Best score: 0.5818452380952381\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training model for target short 8 days\n",
      "\n",
      "Results for model 2000:\n",
      "Best score: 0.5137362637362637\n",
      "Results for model 2001:\n",
      "Best score: 0.5171446986786981\n",
      "Results for model 2002:\n",
      "Best score: 0.5269882285386162\n",
      "Results for model 2003:\n",
      "Best score: 0.5663817663817664\n",
      "Results for model 2004:\n",
      "Best score: 0.5333333333333333\n",
      "Results for model 2005:\n",
      "Best score: 0.6088235294117648\n",
      "Results for model 2006:\n",
      "Best score: 0.5485956990893089\n",
      "Results for model 2007:\n",
      "Best score: 0.48956570783981956\n",
      "Results for model 2008:\n",
      "Best score: 0.49651799687010956\n",
      "Results for model 2009:\n",
      "Best score: 0.5817936148818502\n",
      "Results for model 2010:\n",
      "Best score: 0.5476190476190477\n",
      "Results for model 2011:\n",
      "Best score: 0.5\n",
      "Results for model 2012:\n",
      "Best score: 0.518927782134455\n",
      "Results for model 2013:\n",
      "Best score: 0.5889026402640264\n",
      "Results for model 2014:\n",
      "Best score: 0.5169230769230769\n",
      "Results for model 2015:\n",
      "Best score: 0.5406621392190153\n",
      "Results for model 2016:\n",
      "Best score: 0.5467836257309941\n",
      "Results for model 2017:\n",
      "Best score: 0.6102941176470589\n",
      "Results for model 2018:\n",
      "Best score: 0.5165689149560118\n",
      "Results for model 2019:\n",
      "Best score: 0.5\n",
      "Results for model 2020:\n",
      "Best score: 0.6227529855436832\n",
      "Results for model 2021:\n",
      "Best score: 0.5211921163413701\n",
      "Results for model 2022:\n",
      "Best score: 0.5526872148659174\n",
      "Results for model 2023:\n",
      "Best score: 0.5196516446516447\n",
      "Results for model 2024:\n",
      "Best score: 0.5035903250188964\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training model for target long 10 days\n",
      "\n",
      "Results for model 2000:\n",
      "Best score: 0.5885402639375168\n",
      "Results for model 2001:\n",
      "Best score: 0.5678160919540229\n",
      "Results for model 2002:\n",
      "Best score: 0.5583333333333333\n",
      "Results for model 2003:\n",
      "Best score: 0.6096547314578005\n",
      "Results for model 2004:\n",
      "Best score: 0.5240384615384616\n",
      "Results for model 2005:\n",
      "Best score: 0.6551315789473684\n",
      "Results for model 2006:\n",
      "Best score: 0.564186633039092\n",
      "Results for model 2007:\n",
      "Best score: 0.6021269841269841\n",
      "Results for model 2008:\n",
      "Best score: 0.4234265734265734\n",
      "Results for model 2009:\n",
      "Best score: 0.5641711229946524\n",
      "Results for model 2010:\n",
      "Best score: 0.5876358695652174\n",
      "Results for model 2011:\n",
      "Best score: 0.5022883295194508\n",
      "Results for model 2012:\n",
      "Best score: 0.6207894736842106\n",
      "Results for model 2013:\n",
      "Best score: 0.5123643959301933\n",
      "Results for model 2014:\n",
      "Best score: 0.5857952755905512\n",
      "Results for model 2015:\n",
      "Best score: 0.6586441532258065\n",
      "Results for model 2016:\n",
      "Best score: 0.6512345679012346\n",
      "Results for model 2017:\n",
      "Best score: 0.661873840445269\n",
      "Results for model 2018:\n",
      "Best score: 0.5612735667564448\n",
      "Results for model 2019:\n",
      "Best score: 0.5818330071754729\n",
      "Results for model 2020:\n",
      "Best score: 0.6204081632653061\n",
      "Results for model 2021:\n",
      "Best score: 0.6375696555217831\n",
      "Results for model 2022:\n",
      "Best score: 0.6526785714285714\n",
      "Results for model 2023:\n",
      "Best score: 0.5476825266611977\n",
      "Results for model 2024:\n",
      "Best score: 0.5010293360782295\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training model for target short 10 days\n",
      "\n",
      "Results for model 2000:\n",
      "Best score: 0.5089094796863863\n",
      "Results for model 2001:\n",
      "Best score: 0.5674054160382925\n",
      "Results for model 2002:\n",
      "Best score: 0.49633876761536333\n",
      "Results for model 2003:\n",
      "Best score: 0.5738897637795275\n",
      "Results for model 2004:\n",
      "Best score: 0.45576944475353287\n",
      "Results for model 2005:\n",
      "Best score: 0.5266628644756541\n",
      "Results for model 2006:\n",
      "Best score: 0.5270528813040484\n",
      "Results for model 2007:\n",
      "Best score: 0.5783823529411765\n",
      "Results for model 2008:\n",
      "Best score: 0.5268970293981837\n",
      "Results for model 2009:\n",
      "Best score: 0.5686194311603323\n",
      "Results for model 2010:\n",
      "Best score: 0.569047619047619\n",
      "Results for model 2011:\n",
      "Best score: 0.5\n",
      "Results for model 2012:\n",
      "Best score: 0.5538737446197991\n",
      "Results for model 2013:\n",
      "Best score: 0.589585172109444\n",
      "Results for model 2014:\n",
      "Best score: 0.5898550724637681\n",
      "Results for model 2015:\n",
      "Best score: 0.5257646276595744\n",
      "Results for model 2016:\n",
      "Best score: 0.5918495297805643\n",
      "Results for model 2017:\n",
      "Best score: 0.5626319493314568\n",
      "Results for model 2018:\n",
      "Best score: 0.5524140211640212\n",
      "Results for model 2019:\n",
      "Best score: 0.5215504876649455\n",
      "Results for model 2020:\n",
      "Best score: 0.6040391676866586\n",
      "Results for model 2021:\n",
      "Best score: 0.5859605911330049\n",
      "Results for model 2022:\n",
      "Best score: 0.5350094280326838\n",
      "Results for model 2023:\n",
      "Best score: 0.5090891695369307\n",
      "Results for model 2024:\n",
      "Best score: 0.5059994545950368\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_results, rf_best_models = rolling_model_tuning(X_train, X_test, y_train, y_test, rf_params, rf_pipeline, hyperparameter_tuner='random', n_iter=100, verbose=0, probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results as pickle file in the models folder\n",
    "\n",
    "with open('models/rf_results.pkl', 'wb') as f:\n",
    "    pickle.dump(rf_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "xgb_pipeline = Pipeline(\n",
    "    [('xgb', XGBClassifier())]\n",
    ")\n",
    "\n",
    "# Define the parameters to tune\n",
    "xgb_params = {\n",
    "    'xgb__n_estimators': [100, 200, 300, 400, 500],\n",
    "    'xgb__max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'xgb__learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
    "    'xgb__subsample': [0.5, 0.7, 0.9, 1],\n",
    "    'xgb__colsample_bytree': [0.5, 0.7, 0.9, 1],\n",
    "    'xgb__gamma': [0, 0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    'xgb__reg_alpha': [0, 0.1, 0.5, 1],\n",
    "    'xgb__reg_lambda': [0, 0.1, 0.5, 1],\n",
    "    'xgb__scale_pos_weight': [1, 2, 3, 4, 5],\n",
    "    'xgb__eval_metric': ['mlogloss', 'merror'],\n",
    "    'xgb__objective': ['multi:softmax', 'multi:softprob']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_results, xgb_best_models = rolling_model_tuning(X_train, X_test, y_train, y_test, xgb_params, xgb_pipeline, hyperparameter_tuner='random', n_iter=100, verbose=0, probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results as pickle file in the models folder\n",
    "\n",
    "with open('models/xgb_results.pkl', 'wb') as f:\n",
    "    pickle.dump(xgb_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "svm_pipeline = Pipeline(\n",
    "    [('svm', SVC(probability=True))]\n",
    ")\n",
    "\n",
    "# Define the parameters to tune\n",
    "svm_params = {\n",
    "    'svm__C': [0.1, 1, 10, 100],\n",
    "    'svm__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'svm__degree': [2, 3, 4, 5],\n",
    "    'svm__gamma': ['scale', 'auto'],\n",
    "    'svm__class_weight': ['balanced', None]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_results, svm_best_models = rolling_model_tuning(X_train, X_test, y_train, y_test, svm_params, svm_pipeline, hyperparameter_tuner='random', n_iter=100, verbose=0, probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results as pickle file in the models folder\n",
    "\n",
    "with open('models/svm_results.pkl', 'wb') as f:\n",
    "    pickle.dump(svm_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
