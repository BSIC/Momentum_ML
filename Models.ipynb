{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RepeatedStratifiedKFold, cross_val_score, RepeatedKFold, RandomizedSearchCV, PredefinedSplit\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, confusion_matrix, classification_report, make_scorer, balanced_accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_signals = pd.read_csv('data/indicators.csv', parse_dates=True, index_col='Date')\n",
    "poly_features = pd.read_csv('data/indicators_w_polyterms.csv', parse_dates=True, index_col='Date')\n",
    "labels = pd.read_csv('data/Y_Matrix.csv', parse_dates=True, index_col='Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the period rolling window over which we train new models and how far back the training data goes\n",
    "\n",
    "window = 1\n",
    "lookback = 3\n",
    "\n",
    "# Creating a function to generate the training and testing data for the models, from 2000 onwards we create for every single year a train test split\n",
    "# by using last 3 years as training data and the next year as testing data\n",
    "# Eg. the model that will trade between 2000 and 2001 will be trained on data from 1997 to 1999 and tuned on 1999 to 2000 data (for the purposes of hyperparameter tuning)\n",
    "\n",
    "def generate_data(reg_signals, labels, window, lookback):\n",
    "    '''\n",
    "    Function to generate training and testing data for the models\n",
    "    \n",
    "    Parameters:\n",
    "    reg_signals: DataFrame containing the  signals\n",
    "    labels: DataFrame containing the labels\n",
    "    window: int, number of years to use for the testing data\n",
    "    lookback: int, number of years to use for the training data\n",
    "    \n",
    "    '''\n",
    "    X = reg_signals\n",
    "    y = labels\n",
    "    X_train = []\n",
    "    X_test = []\n",
    "    y_train = []\n",
    "    y_test = []\n",
    "    for i in range(2000, 2025):\n",
    "        date = datetime.strptime(str(i) + '-01-01', '%Y-%m-%d')\n",
    "        X_train.append(X.loc[str(date - pd.DateOffset(years=lookback)):str(date - pd.DateOffset(years=window))])\n",
    "        X_test.append(X.loc[str(date - pd.DateOffset(years=window)):str(date)])\n",
    "        y_train.append(y.loc[str(date - pd.DateOffset(years=lookback)):str(date - pd.DateOffset(years=window))])\n",
    "        y_test.append(y.loc[str(date - pd.DateOffset(years=window)):str(date)])\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = generate_data(reg_signals, labels, window, lookback)\n",
    "X_poly_train, X_poly_test, y_poly_train, y_poly_test = generate_data(poly_features, labels, window, lookback)\n",
    "\n",
    "# NB: y_train and y_poly_train are the same for all models, as the y_backtest and y_poly_backtest are the same for all models, so we just keep one set\n",
    "del y_poly_train, y_poly_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning function for one single pipeline for one single period. We use the same training and testing data for all models to ensure that the results are comparable.\n",
    "# We use the predefined X_train, X_test split instead of cross-validation to avoid look-ahead biases.\n",
    "\n",
    "def tune_model(X_train, X_test, y_train, y_test, params, pipeline, hyperparameter_tuner='grid', n_iter=None, verbose=1, sample_weights=None, probability=True):\n",
    "    '''\n",
    "    Function to tune a model using a predefined split\n",
    "    \n",
    "    Parameters:\n",
    "    X_train: DataFrame, training data\n",
    "    X_test: DataFrame, testing data over which we perform the hyperparameter tuning\n",
    "    y_train: DataFrame, training labels\n",
    "    y_test: DataFrame, testing labels\n",
    "    params: dict, hyperparameters to tune\n",
    "    pipeline: Pipeline, model to tune\n",
    "    hyperparameter_tuner: str, 'grid' or 'random', default='grid'\n",
    "    n_iter: int, number of iterations for random search, default=None\n",
    "    verbose: int, default=1\n",
    "    sample_weights: array-like, default=None\n",
    "    probability: bool, whether the model has a predict_proba method, default=True\n",
    "    \n",
    "    '''\n",
    "    # we first merge the training and testing data\n",
    "    X = pd.concat([X_train, X_test])\n",
    "    y = pd.concat([y_train, y_test])\n",
    "    # our crossvalidation cv uses predefined split to ensure that the training and testing data are actually maintained when tuning hyperparameters:\n",
    "    cv = PredefinedSplit(test_fold=[-1]*len(X_train) + [0]*len(X_test))\n",
    "     \n",
    "    if probability:\n",
    "        scoring = {'auc': make_scorer(roc_auc_score, needs_proba=True, multi_class=\"ovr\"), 'f1_macro': 'f1_macro', 'precision': 'precision_macro', 'recall': 'recall_macro', 'balanced_accuracy': 'balanced_accuracy'}\n",
    "    else: \n",
    "        scoring = {'f1_macro': 'f1_macro', 'precision': 'precision_macro', 'recall': 'recall_macro', 'balanced_accuracy': 'balanced_accuracy'}\n",
    "    principal_metric = 'balanced_accuracy'\n",
    "\n",
    "    if hyperparameter_tuner == 'grid':\n",
    "        search = GridSearchCV(pipeline, params, cv=cv, scoring=scoring, refit=principal_metric, n_jobs=-1, verbose=verbose, error_score='raise')\n",
    "    elif hyperparameter_tuner == 'random':\n",
    "        search = RandomizedSearchCV(pipeline, params, cv=cv, scoring=scoring, refit=principal_metric, n_iter=n_iter, n_jobs=-1, verbose=verbose)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown hyperparameter tuner: {hyperparameter_tuner}\\n Choose from 'grid', 'random'\")\n",
    "    if sample_weights is not None:\n",
    "        kwargs = {pipeline.steps[-1][0] + '__sample_weight': sample_weights}\n",
    "        result = search.fit(X, y, **kwargs)\n",
    "    else:\n",
    "        result = search.fit(X, y)\n",
    "    best_model = result.best_estimator_\n",
    "\n",
    "    return result, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_validation_performance(result, probability=True):\n",
    "    print('')\n",
    "    print(f\"Cross-validation balanced accuracy scores: {result.best_score_}\\n\")\n",
    "    print(f\"Cross-validation F1 Macro scores: {result.cv_results_['mean_test_f1_macro'][result.best_index_]}\\n\")\n",
    "    if probability:\n",
    "        print(f\"Cross-validation ROC AUC scores: {result.cv_results_['mean_test_auc'][result.best_index_]}\\n\")\n",
    "    print(f\"Cross-validation precision scores: {result.cv_results_['mean_test_precision'][result.best_index_]}\\n\")\n",
    "    print(f\"Cross-validation recall scores: {result.cv_results_['mean_test_recall'][result.best_index_]}\\n\")\n",
    "    print(f\"Parameters for the best model: \\n{result.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_model_tuning(X_train, X_test, y_train, y_test, params, pipeline, hyperparameter_tuner='grid', n_iter=None, verbose=1, sample_weights=None, probability=True):\n",
    "    '''\n",
    "    Function to tune a model using a rolling window\n",
    "    \n",
    "    Parameters:\n",
    "    X_train: list, training data\n",
    "    X_test: list, testing data over which we perform the hyperparameter tuning\n",
    "    y_train: list, training labels\n",
    "    y_test: list, testing labels\n",
    "    params: dict, hyperparameters to tune\n",
    "    pipeline: Pipeline, model to tune\n",
    "    hyperparameter_tuner: str, 'grid' or 'random', default='grid'\n",
    "    n_iter: int, number of iterations for random search, default=None\n",
    "    verbose: int, default=1\n",
    "    sample_weights: array-like, default=None\n",
    "    probability: bool, whether the model has a predict_proba method, default=True\n",
    "    \n",
    "    '''\n",
    "    results = []\n",
    "    best_models = []\n",
    "    for i in range(len(X_train)):\n",
    "        result, best_model = tune_model(X_train[i], X_test[i], y_train[i], y_test[i], params, pipeline, hyperparameter_tuner=hyperparameter_tuner, n_iter=n_iter, verbose=verbose, sample_weights=sample_weights, probability=probability)\n",
    "        results.append(result)\n",
    "        best_models.append(best_model)\n",
    "        # we print the results for each model\n",
    "        print(f\"Results for model {i+2000}:\")\n",
    "        if verbose:\n",
    "            model_validation_performance(result, probability=probability)\n",
    "        else:\n",
    "            print(f\"Best score: {result.best_score_}\")\n",
    "    return results, best_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "log_reg_pipeline = Pipeline(\n",
    "    [('log_reg', LogisticRegression(max_iter=1000))]\n",
    ")\n",
    "\n",
    "# Define the parameters to tune\n",
    "log_reg_params = {\n",
    "    'log_reg__C': [0.01, 0.1, 1, 10, 100],\n",
    "    'log_reg__penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    'log_reg__class_weight': ['balanced', None]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py:610: FutureWarning: The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\Users\\Asus\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 428, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"c:\\Users\\Asus\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 275, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Asus\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py\", line 620, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Asus\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n    return [func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Asus\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Asus\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 129, in __call__\n    return self.function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Asus\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\Asus\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Asus\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py\", line 476, in fit\n    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n  File \"c:\\Users\\Asus\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Asus\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n    solver = _check_solver(self.solver, self.penalty, self.dual)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Asus\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n    raise ValueError(\nValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m log_reg_results, log_reg_best_models \u001b[38;5;241m=\u001b[39m rolling_model_tuning(X_train, X_test, y_train, y_test, log_reg_params, log_reg_pipeline, hyperparameter_tuner\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrid\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[1;32mIn[22], line 22\u001b[0m, in \u001b[0;36mrolling_model_tuning\u001b[1;34m(X_train, X_test, y_train, y_test, params, pipeline, hyperparameter_tuner, n_iter, verbose, sample_weights, probability)\u001b[0m\n\u001b[0;32m     20\u001b[0m best_models \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X_train)):\n\u001b[1;32m---> 22\u001b[0m     result, best_model \u001b[38;5;241m=\u001b[39m tune_model(X_train[i], X_test[i], y_train[i], y_test[i], params, pipeline, hyperparameter_tuner\u001b[38;5;241m=\u001b[39mhyperparameter_tuner, n_iter\u001b[38;5;241m=\u001b[39mn_iter, verbose\u001b[38;5;241m=\u001b[39mverbose, sample_weights\u001b[38;5;241m=\u001b[39msample_weights, probability\u001b[38;5;241m=\u001b[39mprobability)\n\u001b[0;32m     23\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[0;32m     24\u001b[0m     best_models\u001b[38;5;241m.\u001b[39mappend(best_model)\n",
      "Cell \u001b[1;32mIn[11], line 44\u001b[0m, in \u001b[0;36mtune_model\u001b[1;34m(X_train, X_test, y_train, y_test, params, pipeline, hyperparameter_tuner, n_iter, verbose, sample_weights, probability)\u001b[0m\n\u001b[0;32m     42\u001b[0m     result \u001b[38;5;241m=\u001b[39m search\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 44\u001b[0m     result \u001b[38;5;241m=\u001b[39m search\u001b[38;5;241m.\u001b[39mfit(X, y)\n\u001b[0;32m     45\u001b[0m best_model \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result, best_model\n",
      "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:968\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    962\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    963\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    964\u001b[0m     )\n\u001b[0;32m    966\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 968\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[0;32m    970\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    972\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1543\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1541\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1542\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1543\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
      "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:914\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    907\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    908\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    909\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    910\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    911\u001b[0m         )\n\u001b[0;32m    912\u001b[0m     )\n\u001b[1;32m--> 914\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    915\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    916\u001b[0m         clone(base_estimator),\n\u001b[0;32m    917\u001b[0m         X,\n\u001b[0;32m    918\u001b[0m         y,\n\u001b[0;32m    919\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m    920\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m    921\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m    922\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[0;32m    923\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[0;32m    924\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[0;32m    925\u001b[0m     )\n\u001b[0;32m    926\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[0;32m    927\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params),\n\u001b[0;32m    928\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit)),\n\u001b[0;32m    929\u001b[0m     )\n\u001b[0;32m    930\u001b[0m )\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    933\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    934\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    935\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    937\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve()\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout))\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m future\u001b[38;5;241m.\u001b[39mresult(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\Lib\\concurrent\\futures\\_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty."
     ]
    }
   ],
   "source": [
    "log_reg_results, log_reg_best_models = rolling_model_tuning(X_train, X_test, y_train, y_test, log_reg_params, log_reg_pipeline, hyperparameter_tuner='grid', verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "rf_pipeline = Pipeline(\n",
    "    [('rf', RandomForestClassifier())]\n",
    ")\n",
    "\n",
    "# Define the parameters to tune\n",
    "rf_params = {\n",
    "    'rf__n_estimators': [100, 200, 300, 400, 500],\n",
    "    'rf__max_depth': [10, 20, 30, 40, 50],\n",
    "    'rf__min_samples_split': [2, 5, 10],\n",
    "    'rf__min_samples_leaf': [1, 2, 4],\n",
    "    'rf__bootstrap': [True, False],\n",
    "    'rf__class_weight': ['balanced', None]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "xgb_pipeline = Pipeline(\n",
    "    [('xgb', XGBClassifier())]\n",
    ")\n",
    "\n",
    "# Define the parameters to tune\n",
    "xgb_params = {\n",
    "    'xgb__n_estimators': [100, 200, 300, 400, 500],\n",
    "    'xgb__max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'xgb__learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
    "    'xgb__subsample': [0.5, 0.7, 0.9, 1],\n",
    "    'xgb__colsample_bytree': [0.5, 0.7, 0.9, 1],\n",
    "    'xgb__gamma': [0, 0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    'xgb__reg_alpha': [0, 0.1, 0.5, 1],\n",
    "    'xgb__reg_lambda': [0, 0.1, 0.5, 1],\n",
    "    'xgb__scale_pos_weight': [1, 2, 3, 4, 5],\n",
    "    'xgb__eval_metric': ['mlogloss', 'merror'],\n",
    "    'xgb__objective': ['multi:softmax', 'multi:softprob']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "svm_pipeline = Pipeline(\n",
    "    [('svm', SVC(probability=True))]\n",
    ")\n",
    "\n",
    "# Define the parameters to tune\n",
    "svm_params = {\n",
    "    'svm__C': [0.1, 1, 10, 100],\n",
    "    'svm__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'svm__degree': [2, 3, 4, 5],\n",
    "    'svm__gamma': ['scale', 'auto'],\n",
    "    'svm__class_weight': ['balanced', None]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
